"""
ai_extractor.py + DEEPSEEK VISION OCR!
‚úÖ –¢–ï–ö–°–¢ –¶–ï–õ–ò–ö–û–ú –±–µ–∑ –æ–±—Ä–µ–∑–∫–∏!
"""

import json
import re
import base64
from typing import Dict, Any, Union, List
from openai import OpenAI
from PIL import Image
import io
import traceback

from logger_cfg import setup_logger
from settings import (
    PERPLEXITY_API_KEY,
    LLM_MODEL,
    HF_API_KEY,
    HF_MODEL,
    LLM_TEMPERATURE,
    LLM_MAX_TOKENS,
    LLM_TIMEOUT,
    MAX_TEXT_FOR_LLM,
)

logger = setup_logger(__name__)

logger.info(f"üîë API_KEY: {'OK' if HF_API_KEY else '‚ùå –ü–£–°–¢–û!'}")
logger.info(f"ü§ñ Text MODEL: {LLM_MODEL}")
logger.info(f"üñºÔ∏è Vision MODEL: {HF_MODEL}")

client = OpenAI(api_key=HF_API_KEY, base_url="https://router.huggingface.co/v1")

def image_to_base64(image: Image.Image) -> str:
    """PIL Image ‚Üí base64"""
    buffer = io.BytesIO()
    image.save(buffer, format='JPEG', quality=95)
    return base64.b64encode(buffer.getvalue()).decode()

def create_deepseek_prompt(page_num: int, total_pages: int) -> str:
    """–ü—Ä–æ–º–ø—Ç DeepSeek Vision OCR"""
    return f"""–ò–∑–≤–ª–µ–∫–∏ –¢–û–ß–ù–´–ô —Ç–µ–∫—Å—Ç –ï–ì–†–ù –≤—ã–ø–∏—Å–∫–∏ —Å —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã. 
–ö–∞–¥–∞—Å—Ç—Ä—ã (74:36:...), –∞–¥—Ä–µ—Å–∞, –ø–ª–æ—â–∞–¥–∏, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤.
–ù–ï –∏—Å–ø—Ä–∞–≤–ª—è–π! –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{total_pages}."""

def extract_text_deepseek_ocr(pdf_images: List[Image.Image]) -> str:
    """üî• DeepSeek Vision OCR - –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã!"""
    full_text = []
    
    for page_num, image in enumerate(pdf_images, 1):
        try:
            logger.info(f"üñºÔ∏è DeepSeek OCR —Å—Ç—Ä.{page_num}/{len(pdf_images)}...")
            
            image_b64 = image_to_base64(image)
            prompt = create_deepseek_prompt(page_num, len(pdf_images))

            messages = [{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
                ]
            }]

            response = client.chat.completions.create(
                model=HF_MODEL,
                messages=messages,
                max_tokens=4096,
                temperature=0.1,
                timeout=60,
            )

            extracted_text = response.choices[0].message.content.strip()
            logger.info(f"  ‚úÖ —Å—Ç—Ä.{page_num}: {len(extracted_text)} —Å–∏–º–≤.")
            full_text.append(extracted_text)
            
        except Exception as e:
            logger.error(f"‚ùå DeepSeek —Å—Ç—Ä.{page_num}: {e}")
            continue
    
    result = "\n\n--- –°–¢–†–ê–ù–ò–¶–ê ---\n\n".join(full_text)
    logger.info(f"üéâ DeepSeek OCR: {len(result)} —Å–∏–º–≤. –≤—Å–µ–≥–æ")
    return result

def extract_content_from_response(response: Any) -> str:
    """üî• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not response:
        return ""
    
    try:
        if hasattr(response, 'choices') and response.choices:
            content = response.choices[0].message.content
            return content.strip()
        
        if isinstance(response, dict) and response.get('choices'):
            content = response['choices'][0]['message']['content']
            return content.strip()
        
        return ""
    except:
        return ""

def call_perplexity_api(pdf_input: Union[str, List[Image.Image]]) -> str:
    """üî• –ì–ò–ë–†–ò–î: DeepSeek OCR ‚Üí Text LLM - –¢–ï–ö–°–¢ –¶–ï–õ–ò–ö–û–ú!"""
    logger.info(f"üåê Text LLM: {LLM_MODEL}")
    
    # –¢–µ—Å—Ç API
    test_response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": "{\"test\": \"OK\"}"}],
        temperature=0.1,
        max_tokens=100
    )
    logger.info(f"‚úÖ –¢–ï–°–¢ API: OK")
    
    # üî• DeepSeek Vision OCR (–µ—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    if isinstance(pdf_input, list) and pdf_input and isinstance(pdf_input[0], Image.Image):
        logger.info("üî• DeepSeek Vision OCR –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω!")
        pdf_text = extract_text_deepseek_ocr(pdf_input)
    else:
        pdf_text = str(pdf_input)
    
    # ‚úÖ –õ–û–ì–ò–†–£–ï–ú –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢!
    logger.info(f"üìÑ –¢–ï–ö–°–¢ –î–õ–Ø LLM: {len(pdf_text)} —Å–∏–º–≤.")
    logger.debug(f"üìÑ Preview: {pdf_text[:500]}...")
    
    # ‚úÖ –ï–°–õ–ò –¢–ï–ö–°–¢ –û–ß–ï–ù–¨ –ë–û–õ–¨–®–û–ô - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏!
    if len(pdf_text) > 32000:  # –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
        logger.warning(f"‚ö†Ô∏è –¢–µ–∫—Å—Ç {len(pdf_text)} —Å–∏–º–≤. ‚Üí —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞—Å—Ç–∏!")
        chunks = [pdf_text[i:i+32000] for i in range(0, len(pdf_text), 32000)]
        results = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"üìÑ –ß–∞—Å—Ç—å {i+1}/{len(chunks)}: {len(chunk)} —Å–∏–º–≤.")
            chunk_result = _process_text_chunk(chunk)
            results.append(chunk_result)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        pdf_text = "\n\n".join([r for r in results if r])
        logger.info(f"üìÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(pdf_text)} —Å–∏–º–≤.")
    
    prompt = f"""–ò–ó–í–õ–ï–ö–ò –∏–∑ –ï–ì–†–ù –¢–û–õ–¨–ö–û JSON!

–ü–†–ê–í–ò–õ–ê:
- –¢–û–õ–¨–ö–û JSON! –ë–µ–∑ —Ç–µ–∫—Å—Ç–∞/–æ–±—ä—è—Å–Ω–µ–Ω–∏–π!
- –ù–µ –Ω–∞—à—ë–ª ‚Üí null
- –ö–∞–¥–∞—Å—Ç—Ä: –í–°–ï 74:36:...
- –ü–ª–æ—â–∞–¥—å: –ß–ò–°–õ–û (1234.56)
- –î–∞—Ç—ã: –î–î.–ú–ú.–ì–ì–ì–ì

{{
  "cadastral_number": null,
  "cadastral_building": null,
  "address": null,
  "area": null,
  "owner": null,
  "tenant": null,
  "floor": null,
  "literal": null,
  "cadastral_quarter": null
  "permitted_use": null,
  "room_number": null,
  
  "status": null,
  "rental_data", {{
      "rent_type": null,
      "period_start": null,
      "period_end": null
  }} 
}}

–¢–ï–ö–°–¢:
{pdf_text}

JSON:"""

    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1,
        max_tokens=LLM_MAX_TOKENS,
        timeout=LLM_TIMEOUT,
    )
    
    api_response = extract_content_from_response(response)
    logger.info(f"‚úÖ Text LLM: {len(api_response)} —Å–∏–º–≤.")
    return api_response or '{"error": "Empty response"}'

def _process_text_chunk(chunk: str) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç –ø–æ —á–∞—Å—Ç—è–º"""
    prompt = f"""–ù–∞–π–¥–∏ –≤ –ï–ì–†–ù –¢–ï–ö–°–¢–ï:
- –ö–∞–¥–∞—Å—Ç—Ä—ã 74:36:...
- –ê–¥—Ä–µ—Å–∞ (–≥., —É–ª., –¥.)
- –ü–ª–æ—â–∞–¥–∏ (—á–∏—Å–ª–∞)
- –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –Ω–∞–π–¥–µ–Ω–Ω–æ–µ:"""
    
    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": f"{prompt}\n\n{chunk[:3000]}" }],
        max_tokens=2000,
        temperature=0.1,
    )
    return extract_content_from_response(response)

def parse_json_response(json_str: str) -> Dict[str, Any]:
    """üî• –ü–∞—Ä—Å–µ—Ä JSON"""
    logger.info(f"üîç RAW JSON ({len(json_str)} —Å–∏–º–≤.): {json_str[:300]}...")
    
    if not json_str:
        return {"error": "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç"}
    
    # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    cleaned = re.sub(r'``````', '', json_str)
    cleaned = re.sub(r'\n\s+', ' ', cleaned).strip()
    
    try:
        parsed = json.loads(cleaned)
        logger.info(f"‚úÖ JSON: {list(parsed.keys())}")
        logger.info(f"   –ö–∞–¥–∞—Å—Ç—Ä: {parsed.get('cadastral_number')}")
        logger.info(f"   –ê–¥—Ä–µ—Å: {parsed.get('address')}")
        logger.info(f"   –ü–ª–æ—â–∞–¥—å: {parsed.get('area')}")
        return parsed
    except json.JSONDecodeError:
        # Emergency –ø–∞—Ä—Å–∏–Ω–≥
        cadastral = re.search(r'cadastral[_-]?number["\s:]*"?([^\s,"}]*)"?', cleaned, re.I)
        address = re.search(r'address["\s:]*"?([^\s,"}]*)"?', cleaned, re.I)
        area = re.search(r'"?area["\s:]*"?([0-9.,]+)"?', cleaned)
        
        emergency = {
            "cadastral_number": cadastral.group(1) if cadastral else None,
            "address": address.group(1) if address else None,
            "area": float(area.group(1).replace(',', '.')) if area else None,
            "emergency_parsed": True
        }
        logger.info(f"‚úÖ Emergency: {emergency}")
        return emergency

def extract_egrn_data(pdf_input: Union[str, List[Image.Image]]) -> Dict[str, Any]:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info(f"üìÑ extract_egrn_data: {len(str(pdf_input))} —Å–∏–º–≤./–∏–∑–æ–±—Ä.")
    
    if not pdf_input:
        return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"}
    
    try:
        json_str = call_perplexity_api(pdf_input)
        data = parse_json_response(json_str)
        logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢: {json.dumps(data, ensure_ascii=False, indent=2)}")
        return {"data": data}
    except Exception as e:
        logger.error(f"‚ùå extract_egrn_data: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    test_text = """–ö–∞–¥–∞—Å—Ç—Ä–æ–≤—ã–π –Ω–æ–º–µ—Ä: 74:36:0303005:71
–ê–¥—Ä–µ—Å: –ß–µ–ª—è–±–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –≥ –ß–µ–ª—è–±–∏–Ω—Å–∫, —Ä-–Ω –õ–µ–Ω–∏–Ω—Å–∫–∏–π, —É–ª –ï–Ω–∏—Å–µ–π—Å–∫–∞—è
–ü–ª–æ—â–∞–¥—å: 6345"""
    
    result = extract_egrn_data(test_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))
